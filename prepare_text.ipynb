{
 "metadata": {
  "name": "",
  "signature": "sha256:20d53c98b0cc8ca961e7691527754fcfdc5cbff7f685e63f3c68151cb0ab9a87"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Clean up and prepare the text for ARTM"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim, nltk, re, pprint\n",
      "from gensim import corpora, models, utils\n",
      "from nltk import WordNetLemmatizer\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopWords = stopwords.words('english')\n",
      "\n",
      "# for Russian:\n",
      "# stopWords = stopwords.words('russian') \n",
      "\n",
      "# to extend the list of stopwords:\n",
      "# more_sw = []\n",
      "# stopWords = set(stopWords + more_sw)\n",
      "\n",
      "print('Number of stopwords:',len(stopWords))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean(word):\n",
      "    word = re.sub(\"(\\@[A-Za-z0-9]*)|(www\\.[A-Za-z0-9]*)|([\\.\\-\\/!\\?%$\\:;#,&]+)|(\\/\\/t\\.co[A-Za-z0-9]*)|([.\\(\\)][A-Za-z0-9]*)|(http[s]?:\\/\\/[A-Za-z0-9]*)|([A-Za-z]*[0-9][A-Za-z]*)|(\\.,\\'\\\"\\(\\))\", '', word)\n",
      "    word = nltk.stem.WordNetLemmatizer().lemmatize(word.lower())\n",
      "    return word\n",
      "\n",
      "# enter a name of the file with extracted tweets; encoding='ISO-8859-15' is usually for English, encoding='utf-8' for Russian\n",
      "raw = []\n",
      "with open('', 'wr', encoding='ISO-8859-15') as f: ian\n",
      "    for line in f:\n",
      "        raw.append(line)\n",
      "        \n",
      "print('Number of documents:',len(raw))\n",
      "docs = [[clean(w) for w in word_tokenize(text) if clean(w) not in stopWords] for text in raw]\n",
      "dictionary = gensim.corpora.Dictionary(w for w in docs)\n",
      "print(\"Number of words in dictionary:\",len(dictionary))\n",
      "for i in range(10):\n",
      "    print(dictionary[i])\n",
      "\n",
      "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
      "for i in range(10):\n",
      "    print(corpus[i])\n",
      "\n",
      "tf_idf = gensim.models.TfidfModel(corpus)\n",
      "print(tf_idf)\n",
      "\n",
      "# enter a filename in format 'docword.*.txt'\n",
      "with open('', 'w') as f:\n",
      "    f.write(str(len(raw))+'\\n')\n",
      "    f.write(str(len(dictionary))+'\\n')\n",
      "    f.write(str((tf_idf.num_nnz))+'\\n')\n",
      "    for i in range(len(raw)):\n",
      "        for w in corpus[i]:\n",
      "            f.write(str(i+1)+ ' ')\n",
      "            f.write(str(w[0]+1)+' '+str(w[1]))\n",
      "            f.write('\\n')        \n",
      "            \n",
      "# enter a filename in format 'vocab.*.txt'\n",
      "with open('', 'w') as f:\n",
      "        f.write('qwertyuiop'.encode('ISO-8859-15')+'\\n')\n",
      "        for w in range(1, len(dictionary)):\n",
      "            f.write(dictionary[w].encode('ISO-8859-15')+'\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
