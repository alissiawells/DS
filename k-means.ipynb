{
 "metadata": {
  "name": "",
  "signature": "sha256:f59e88057d2a7366c82fb637f91446f2068f5c36d3d4e067513d931262119e02"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim\n",
      "import nltk\n",
      "import os\n",
      "import codecs\n",
      "from gensim import corpora, models, utils\n",
      "import pymorphy2\n",
      "from nltk.tokenize import word_tokenize\n",
      "import re\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.cluster import KMeans"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = pymorphy2.MorphAnalyzer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopWords = set(line.strip() for line in open('swrussian.txt', 'r'))\n",
      "#the list of stop words can be expanded"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean(line):\n",
      "     a = re.search('(http[s]?:\\/\\/yandex\\.ru\\/search\\/\\?text=).+(http[s]?)', line)\n",
      "     a = a.group(0)\n",
      "     a = re.sub('(http[s]?:\\/\\/yandex\\.ru\\/search\\/\\?text=)|(&.+)|(http[s]?:\\/\\/.+)|([A-Za-z0-9]+)', ' ', a)\n",
      "     return(a).strip()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lemm(word):\n",
      "    word = re.sub(\"(\\@[A-Za-z0-9]*)|(http[s]?:\\/\\/[A-Za-z0-9]*)|(www\\.[A-Za-z0-9]*)|([\\(\\)=_\\'\\\"\\.,\\-\\/!\\?%$\\:;#&]+)|(\\/\\/t\\.co[A-Za-z0-9]*)|(rusearchtext=)|(client=)|(flag=)\", '', word)\n",
      "    return m.parse(word.lower())[0].normal_form.strip()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nameofdoc = '.txt'\n",
      "true_k ="
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open(nameofdoc, 'r', encoding='utf-8') as f:\n",
      "    s = f.readlines()\n",
      "\n",
      "i = 1\n",
      "err = []\n",
      "with codecs.open('clean%s' % nameofdoc, 'w', encoding='utf-8') as f:\n",
      "    while i < len(s):\n",
      "        try:\n",
      "            f.write(clean(s[i])+'\\n')\n",
      "        except AttributeError:\n",
      "            err.append(s[i])\n",
      "        i += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('clean%s' % nameofdoc, 'r', encoding='utf-8') as f: \n",
      "    raw = f.readlines()  \n",
      "print('Number of documents:',len(raw))\n",
      "\n",
      "docs = [[lemm(w).encode('utf-8') for w in word_tokenize(text) if lemm(w).encode('utf-8') not in stopWords] for text in raw]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_words = []\n",
      "for doc in docs:\n",
      "    all_words += doc\n",
      "print('\u0421\u0430\u043c\u044b\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430:')\n",
      "loopa = Counter(all_words).most_common()[:50]\n",
      "for poopa in loopa:\n",
      "    print poopa[0], poopa[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_tfidf = []\n",
      "for doc in docs:\n",
      "    data_tfidf.append(' '.join(doc))\n",
      "\n",
      "vectorizer = TfidfVectorizer()\n",
      "tf_idf = vectorizer.fit_transform(data_tfidf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
      "model.fit(tf_idf)\n",
      " \n",
      "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
      "terms = vectorizer.get_feature_names()\n",
      "for i in range(true_k):\n",
      "    print 'Cluster %d:\\n' % i\n",
      "    for ind in order_centroids[i, :10]:\n",
      "        print ' %s' % terms[ind]\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "labels = model.labels_\n",
      "df = pd.DataFrame({'clust': labels, 'text': docs})\n",
      "dff = df.groupby('clust').count()\n",
      "percent = []\n",
      "for i in range(true_k):\n",
      "    a = dff['text'][i] *100 / len(raw)\n",
      "    percent.append(a)\n",
      "dff['percent'] = percent\n",
      "dff"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
